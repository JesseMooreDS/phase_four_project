{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a041ccf-d395-4133-a027-34966840e0a5",
   "metadata": {},
   "source": [
    "# Top 5 Flicks - Movie Recommendation System\n",
    "- Author: Jesse Moore\n",
    "- Phase 4\n",
    "- Instructor - Mark Barbour\n",
    "- Blog Post: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f8903-927e-4206-b79c-021f8f193b4b",
   "metadata": {},
   "source": [
    "![image](images/picking_movie.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0346e0-4521-40fa-8dac-c5f58b4bc727",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147732e5-e00e-4cde-a5c1-1bf75636af43",
   "metadata": {},
   "source": [
    "Our project aims to develop a recommendation system for a new streaming service, providing users with five personalized movie recommendations. The model that we developed uses both K-nearest Neighbors (KNN) and Alternating Least Squares modeling to create our recommendation system and to provide users with 5 recommendations. \n",
    "\n",
    "Business and Data Understanding:\n",
    "We utilized the MovieLens dataset, which contains 100,000 ratings and 3,600 tags, making it well-suited for training a recommendation system. This dataset provides rich user-movie interactions and metadata, essential for building effective recommendations.\n",
    "\n",
    "Data Preparation:\n",
    "For our modeling, the data required little / no cleaning or preparation aside from merging our ratings data with our movie data. \n",
    "\n",
    "Libraries: \n",
    "Aside from the standard libraries (numpy, pandas, random, matplotlib.pyplot, seaborn, warnings), we used the following libraries: zipfile for extracting our data; scipy for csr_matrices, scikit-learn for model selection, label encoding, and evaluating cosine similarity; pyspark for Alternate Least Squares (ALS) modelling; and Surprise for our K-nearest Neighbors modelling. \n",
    "\n",
    "Modeling:\n",
    "We initially implemented an Alternating Least Squares (ALS) model using Spark for collaborative filtering. ALS decomposes the user-item matrix to uncover latent factors, optimized via hyperparameter tuning (rank, maxIter, and RegParam). We then introduced a K-Nearest Neighbors (KNN) baseline model using the Surprise package, tuning for optimal k and min_k values. This approach strategizes to combat 'cold-start' problems where users have little to no rating history, or when movies have little to no ratings. \n",
    "\n",
    "Evaluation:\n",
    "Performance was assessed using Root Mean Squared Error (RMSE), where a lower RMSE indicates better predictive accuracy. Grid search tuning found the best model to be KNN Baseline (k=56, min_k=14) with an RMSE of 0.886, slightly outperforming the ALS baseline (RMSE 0.888). ALS remains valuable for handling cold start issues.\n",
    "\n",
    "Insights and Limitations:\n",
    "Additional user behavior tracking—such as viewing duration and repeated watches—could improve recommendations. Incorporating explicit user preferences, such as favorite genres and actors, would further refine personalization. Future iterations may explore hybrid deep learning models for enhanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c1e6b-de71-414b-9f6d-527ccd1fae57",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbcc3b-45cf-465d-9755-7a9d98315ad7",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bffdef-bc35-435e-9d95-59e2e293dff0",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68621aac-2827-4009-a2c9-96dda4049dd2",
   "metadata": {},
   "source": [
    "Our client is launching a new streaming service and wants to implement a recommendation system that provides users with five personalized movie recommendations.\n",
    "\n",
    "To achieve this, we will use a hybrid approach, combining collaborative filtering and content-based filtering. This will help mitigate the cold start problem, which occurs when:\n",
    "\n",
    "New users, who haven’t rated any content, receive poor or no recommendations.\n",
    "Movies with few or no ratings are unlikely to be recommended.\n",
    "Additionally, we have been tasked with delivering insights into user engagement, enabling our client to maximize engagement with their growing user base.\n",
    "\n",
    "Evaluation Metrics\n",
    "To assess the effectiveness of our recommendation system, we will use the following key metrics:\n",
    "\n",
    "Cosine Similarity – Measures the similarity between movies based on content features. It calculates the angle between feature vectors, helping identify movies with similar characteristics.\n",
    "\n",
    "Root Mean Squared Error (RMSE) – Evaluates the accuracy of our rating predictions by measuring the average difference between actual and predicted ratings. A lower RMSE indicates better prediction performance.\n",
    "By leveraging this hybrid approach and these evaluation metrics, we aim to build a recommendation system that delivers high-quality suggestions while providing valuable insights for our client.\n",
    "\n",
    "These concepts will be explained in further detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8c493-de6b-4a72-8ae1-98dd335f02f4",
   "metadata": {},
   "source": [
    "### Business Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498ad96-0c72-49d6-9b2f-20fa7745967c",
   "metadata": {},
   "source": [
    "Our business objective is to enhance user enjoyment and engagement with our client's streaming service.\n",
    "\n",
    "To achieve this, we will recommend five movies to each user that they are likely to enjoy. Success will be measured by ensuring that our recommendations receive higher rating scores compared to a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9fdd2-b253-4728-9028-11506bb80d9e",
   "metadata": {},
   "source": [
    "### Stakeholder Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5955b16-6ea1-4e6a-b8d5-0cd3ce44481f",
   "metadata": {},
   "source": [
    "User Behavior & Engagement\n",
    "*What types of movies (genres, ratings, release years) are most frequently watched?\n",
    "*Are there specific times or days when users are more active on the platform?\n",
    "\n",
    "Content Optimization\n",
    "What are the most popular movies among different audience segments?\n",
    "Are there under-watched but highly-rated movies that we should promote?\n",
    "What content categories (e.g., drama, comedy, action) drive the most engagement?\n",
    "Is there a difference in engagement between older classics and new releases?\n",
    "\n",
    "\n",
    "Personalization & Recommendations\n",
    "What patterns exist in user ratings that can inform better recommendations?\n",
    "Can we predict what a user might watch next based on their history?\n",
    "Should we prioritize recommending high-rated content or content that aligns with past behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018b2a2-2785-40e8-8cdf-435a7a0cd961",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272065f-3a53-4257-bd1f-c21afe1dfd6e",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70824f-6f91-4fee-a705-67df9bd5a3fd",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515ad6d-bd95-4560-89fb-0b354e053c9f",
   "metadata": {},
   "source": [
    "Our dataset is the [MovieLens](https://grouplens.org/datasets/movielens/latest/) dataset that has been created by the GroupLens research lab at the University of Minnesota. This dataset contains 100,000 ratings, 3,600 tags and was last updated on 9/2018. While the dataset contains several csv files of data, such as 'tags.csv' (which contains user created tags for movies) and 'link.csv' (which contains keys to link our data with imdb and tmdb information), we will not be utilizing this data, instead focusing on the 'movies.csv' and 'ratings.csv' datafiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2ce73-aec8-435e-804a-d0e18988bfee",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddf47e-0ba5-4247-80d8-d7201503a4e1",
   "metadata": {},
   "source": [
    "movies.csv - contains the nearly 10,000 movies that have been rated, their title and genre.\n",
    "\n",
    "- movieId - the Id of the movie, this will used to be merge this information with other data. \n",
    "- title - the title of the movie, also generally contains the year of the movie.\n",
    "- genres -  the genres that the movie comprises. This data will be transformed later, to split genre tags such as 'Action/Adventure/Animation' into their individual components, and will be explained later in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77a525-780f-4dbb-9273-22d139b59a48",
   "metadata": {},
   "source": [
    "ratings.csv - includes the over 100,000 user ratings.\n",
    "\n",
    "- userId - the Id of the user who left the rating, this will be used to merge with other data.\n",
    "- movieId - the Id of the movie, we will use this Id to merge with other data.\n",
    "- rating - the rating, ranging from 0.5 to 5.0.\n",
    "- timestamp - the time when the rating was left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369e8c3-e902-41d7-8329-14e4a93760cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Processing & Utilities\n",
    "from zipfile import ZipFile\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Machine Learning & Model Evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Apache Spark for Large-Scale Processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode, rank, log2\n",
    "\n",
    "# Spark Machine Learning (ALS Recommender System)\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# Surprise Library for Collaborative Filtering\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise.prediction_algorithms import KNNBaseline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bf8a0-ea57-4367-8aee-937f99b98d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality of Life (QoL) Enhancements for Reproducibility, Plotting, and Warnings\n",
    "\n",
    "# Set a random seed for reproducibility across random number generators\n",
    "random_seed = 42  \n",
    "np.random.seed(random_seed)  \n",
    "random.seed(random_seed)  \n",
    "\n",
    "# Set plotting style for better visuals in charts\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  \n",
    "\n",
    "# Suppress all warnings to avoid clutter in output\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "# Disable warnings for chained assignments in pandas (prevents 'SettingWithCopyWarning')\n",
    "pd.options.mode.chained_assignment = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7507d98-be79-4230-897e-36f50de25b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MovieLens Database and Extracting Data\n",
    "\n",
    "# Define file paths\n",
    "zip_file_path = 'data/ml-latest-small.zip'  \n",
    "extract_folder = 'data/'\n",
    "\n",
    "# Extract the dataset from the zip file\n",
    "with ZipFile(zip_file_path, 'r') as zip_ref:  \n",
    "    zip_ref.extractall(extract_folder)  \n",
    "\n",
    "# Load the datasets into pandas DataFrames\n",
    "datasets = {\n",
    "    'movies': 'movies.csv', \n",
    "    'ratings': 'ratings.csv'\n",
    "}\n",
    "\n",
    "# Using a loop to load all datasets at once\n",
    "dataframes = {}\n",
    "for key, file_name in datasets.items():\n",
    "    dataframes[key] = pd.read_csv(f'data/ml-latest-small/{file_name}', encoding='utf-8')\n",
    "\n",
    "# Access individual dataframes as needed, for example:\n",
    "movies_df = dataframes['movies']\n",
    "ratings_df = dataframes['ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dc3b2-a39d-4d85-9c2d-9f3497abdacf",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbea7f2-2e93-419a-8d2e-cb78e638e1a4",
   "metadata": {},
   "source": [
    "There was little data cleaning that was done in this notebook, as we only utilized the movies.csv and ratings.csv for our modeling. This data was organized and needed no cleaning. (Note - in our exploratory notebook we have done more extensive cleaning that can be used to implement further testing in the future, see Limitations). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b02e78-88e1-45da-a909-6b87b5542557",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8b101-1f31-4374-9928-f397d578eae0",
   "metadata": {},
   "source": [
    "There was little data cleaning that was done in this notebook, as we only utilized the movies.csv and ratings.csv for our modeling. We chose to In future implementations of our model, removing users who have left too few ratings, or movies that have too few ratings can considered to be removed from our modelling data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da545221-7aa2-465e-bb0c-d794820ab6c0",
   "metadata": {},
   "source": [
    "### Matrix Sparsity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9056e-aeaf-4d0d-bc6c-5155d8bc5354",
   "metadata": {},
   "source": [
    "Before we can build our hybrid recommender system, we must analyze the matrix sparsity to ensure that the majority of our users have rated more than very few items. While we will build a hybrid recommender system that factors for cold-start issues, we must also contend with a model that will not be able to generalize predictions if there is too much sparsity. Too little, and we may have bias in our dataset. \n",
    "\n",
    "Sparsity is calculated as: sparsity = 1 - no# of non-zero interactions / total possible interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd29a3e-da89-4aec-9b4e-843b6d74e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3f8f3-b6f8-43df-821f-cd8d17f68f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['userId_x'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "ratings_df['movieId_x'] = item_encoder.fit_transform(ratings_df['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95039bb-062b-46ea-994d-f6ba64e5c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings_df['userId_x'].nunique()\n",
    "num_movies = ratings_df['movieId_x'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5840d-cd82-4125-944e-9efa37a009cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix = csr_matrix(\n",
    "    (ratings_df['rating'], (ratings_df['userId_x'], ratings_df['movieId_x'])),\n",
    "    shape=(num_users, num_movies)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495b824-43d8-4ffd-95f8-eaa0a865392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Created sparse matrix with shape: {interaction_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe38db-88b6-4a99-bd3f-053caa40b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = interaction_matrix.nnz\n",
    "sparsity = 1 - (num_interactions / (num_users * num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3274b7-b620-46dc-bbfc-51f16b722d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sparsity of the interaction matrix: {sparsity:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16c643-311d-47ee-886a-1f345efd60f2",
   "metadata": {},
   "source": [
    "The results of our matrix sparsity analysis indicates that are dataset is a highly sparse dataset with shape: (610, 9724) and the sparsity of the interaction matrix is 98.30%. Because of the highly sparse nature of our dataset, and because we have explicit ratings, we will utilize a hybrid recommendation system using both KNN and ALS models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd68b70-ee2c-41d8-b15e-8a27690649b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions = np.array(interaction_matrix.sum(axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23af9ad-c9a9-4c07-b3e2-1641580b6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(user_interactions, bins=20, log=True, color='blue', alpha=0.7)\n",
    "plt.xlabel('Number of Interactions per User')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.title('User Interaction Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605957d5-dcd8-4e2c-bf7e-aa6be5afe35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_interactions = np.array(interaction_matrix.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7136fc-7fc7-4ca5-b2e7-5c1221617cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(item_interactions, bins=50, log=True, color='blue', alpha=0.7)\n",
    "plt.xlabel('Number of Interactions per Item')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.title('Item Interaction Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6fa74-3b91-4fb4-a91c-f258c1e26bf7",
   "metadata": {},
   "source": [
    "This 'long-tail' distribution shows us that the majority of our movies have fewer or no ratings/interactions, while a small amount of popular have a large number of interactions. This can lead to cold-start problems, when the system fails to recommend an item because it has too little information to go on. We will use a hybrid recommendation system to account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a6466-54fd-4a04-a9b9-e85ceaa1fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_users = 200\n",
    "sample_items = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa8594-fe31-444c-883a-b077b225646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(interaction_matrix[:sample_users, :sample_items].toarray(), cmap='YlGnBu', cbar=False)\n",
    "plt.xlabel('Items')\n",
    "plt.ylabel('Users')\n",
    "plt.title('Interaction Matrix Heatmap (Subset)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75856246-4de7-414d-be77-b72e0cd5f578",
   "metadata": {},
   "source": [
    "Since the majority of our heatmap shows off-white, this represents 0 or near-zero values in our dataset, while the darker zones represent larger values. This indicates a highly sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd34d9-ec58-4cbf-b9b8-b3b62fca3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_users = np.sum(interaction_matrix, axis=1) == 0\n",
    "cold_items = np.sum(interaction_matrix, axis=0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f86a7-7f46-4ece-979e-f074b8dfee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cold-start users: {np.sum(cold_users)} / {interaction_matrix.shape[0]}')\n",
    "print(f'Cold-start items: {np.sum(cold_items)} / {interaction_matrix.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385a193-a5dd-44f2-86e6-ad93b72cbb44",
   "metadata": {},
   "source": [
    "This shows us that, currently, every user has interacted with at least one item, and every item has been interacted with by at least one user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516cf1f-b4cd-469e-8fd4-c7c30810843f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3331c6-a2a4-408e-abfd-0c3b4fcafb76",
   "metadata": {},
   "source": [
    "We began with Alternating Least Squares (ALS) using Apache Spark as our baseline model. ALS is a collaborative filtering method that factorizes the user-item matrix into two smaller matrices, capturing latent factors of users and items. The model iteratively optimizes one matrix while fixing the other, minimizing the least squared error. We tuned the following hyperparameters:\n",
    "\n",
    "* rank (number of latent factors),\n",
    "* maxIter (maximum iterations for optimization),\n",
    "* RegParam (regularization to prevent overfitting).\n",
    "\n",
    "While ALS effectively handles the cold-start problem by learning from implicit feedback, it struggled with accuracy for well-established users with substantial rating histories.\n",
    "\n",
    "To improve recommendations, we implemented a K-Nearest Neighbors (KNN) Baseline model using Surprise. KNN finds the K most similar users or items and predicts ratings based on their aggregated preferences. We tuned:\n",
    "\n",
    "* k (number of neighbors considered),\n",
    "* min_k (minimum neighbors required for aggregation).\n",
    "\n",
    "To optimize both models, we performed grid search, systematically testing hyperparameter combinations to find the best-performing configuration. Our final KNN Baseline model (with k=56 and min_k=14) achieved an RMSE of 0.886, outperforming ALS (RMSE = 0.888). The improvement, while marginal, suggests KNN's advantage in leveraging explicit user rating similarities, making it more effective for established users.\n",
    "\n",
    "However, KNN struggles with sparsity, making ALS a valuable fallback for new users or unrated items. A hybrid system leveraging both models balances accuracy and cold-start mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a29888-3373-4bf9-a112-b5bfa9209ced",
   "metadata": {},
   "source": [
    "# Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c28f9-a4f8-4c0b-b4a3-78d3cce73130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName('MoieRecommender').getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f12b2c-5bea-4343-b107-a76b7d353b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading our dataset using spark.\n",
    "spark_df = spark.read.csv('data/ml-latest-small/ratings.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3940c6-711b-41fd-b29d-b8b6223c7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation, training and hold out test sets.\n",
    "# creating training and validation sets\n",
    "train, validation, hold_out = spark_df.randomSplit([0.64, 0.16, 0.2], seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cf248-768a-40cd-a1dc-9ab198e32f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our ALS (Spark) model.\n",
    "als = ALS(\n",
    "    userCol='userId',\n",
    "    itemCol='movieId',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4c0a9-6828-4ee6-bd61-fab7d96d845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our parameter grid for our ALS spark model\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, [12, 13, 14])\\\n",
    "    .addGrid(als.maxIter, [18, 19, 20])\\\n",
    "    .addGrid(als.regParam, [.17, .18, .19])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae963d-01de-4fe6-a562-130b08358249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Regression Evaluator using RMSE as our metric.\n",
    "\n",
    "reg_evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol ='rating',\n",
    "    predictionCol='prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717db516-3794-43f1-8b7a-f19247edbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using Train Validation Split\n",
    "\n",
    "train_val_split = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator= reg_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903db140-7f28-4e98-87df-96572a439f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our ALS model\n",
    "spark_model = train_val_split.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3b305-5f0b-4f2c-ad13-11b43c669440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best model from the tuning process using ParamGridBuilder\n",
    "best_model = spark_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edc2e7-729d-481e-8679-46b02d82767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = spark_model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e39f8-f21f-45e5-9657-22f72162142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RMSE using our prediction and reg_evaluator\n",
    "rmse = reg_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebbcb0-2080-4e32-9e12-ec2b4b036aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"  **BEST MODEL**\n",
    "  RSME = {rmse}\n",
    "  Hyperparameters:\n",
    "  Rank: {best_model.rank}\n",
    "  Max Iterations: {best_model._java_obj.parent().getMaxIter()}\n",
    "  Regularization Parameter: {best_model._java_obj.parent().getRegParam()}\n",
    "  Nonnegative: {best_model._java_obj.parent().getNonnegative()}\n",
    "  Cold Start Strategy: {best_model._java_obj.parent().getColdStartStrategy()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d3e9c-7ceb-452d-8f87-29c33b73d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol='userId',\n",
    "    itemCol='movieId',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative= True,\n",
    "    rank= 30,\n",
    "    maxIter= 30,\n",
    "    regParam= .18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4116005-ce31-43ad-b0ce-9aeca9d96ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our parameter grid for our ALS spark model\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, [20, 30, 40])\\\n",
    "    .addGrid(als.maxIter, [5, 10, 15])\\\n",
    "    .addGrid(als.regParam, [.10, .20, .30])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14979f-9711-456f-86d2-9bfa3b9d6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using Train Validation Split\n",
    "\n",
    "train_val_split = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator= reg_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761916a-8718-4e51-b0b8-1cfee621b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our ALS model\n",
    "spark_model = train_val_split.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33346f17-25a5-41e1-9367-abfb29e9b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best model from the tuning process using ParamGridBuilder\n",
    "best_model = spark_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186df68-ead6-4e8e-8154-964460e28b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = spark_model.transform(validation)\n",
    "\n",
    "# Generate RMSE using our prediction and reg_evaluator\n",
    "rmse = reg_evaluator.evaluate(predictions)\n",
    "\n",
    "# Printing our evaluation metrics and model parameters\n",
    "# Printing our evaluation metrics and model parameters\n",
    "print(f\"\"\"  RSME = {rmse} of current model.\n",
    "  **BEST MODEL**\n",
    "  Hyperparameters:\n",
    "  Rank: {best_model.rank}\n",
    "  Max Iterations: {best_model._java_obj.parent().getMaxIter()}\n",
    "  Regularization Parameter: {best_model._java_obj.parent().getRegParam()}\n",
    "  Nonnegative: {best_model._java_obj.parent().getNonnegative()}\n",
    "  Cold Start Strategy: {best_model._java_obj.parent().getColdStartStrategy()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433399c2-060e-424a-8a9b-b08c62122437",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0143fd7-6ae8-43c0-9251-058d1cc85542",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "expected_column_names = [\"userId\", \"movieId\", \"rating\"]\n",
    "\n",
    "# Load the data into a Surprise Dataset\n",
    "data_surp = Dataset.load_from_df(ratings_df[expected_column_names], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89f7e5-5c3a-4af0-9ca1-b1ef9939880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validating with KNNBaseline\n",
    "knn_baseline = KNNBaseline(sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_baseline = cross_validate(knn_baseline, data_surp, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf5e47-1f7b-4f79-a73a-1b5a08ffb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cv_knn_baseline.items():\n",
    "    print(i)\n",
    "    \n",
    "print('-----------------------')\n",
    "# print validation results\n",
    "np.mean(cv_knn_baseline['test_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d143f-da38-4836-a994-aa163d6e24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK it seems that our best rmse scores come from the knn_baselin model with the hyperparameter values k=56 and min_k=14 so tha is what we will use as our final model.\n",
    "# cross validating with KNNBaseline\n",
    "final_knn_baseline = KNNBaseline(k=56, min_k=14, sim_options={'name':'pearson', 'user_based':True})\n",
    "final_cv_knn_baseline = cross_validate(final_knn_baseline, data_surp, n_jobs=-1)\n",
    "\n",
    "for i in cv_knn_baseline.items():\n",
    "    print(i)\n",
    "    \n",
    "print('-----------------------')\n",
    "# print validation results\n",
    "np.mean(final_cv_knn_baseline['test_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161c609-ceff-4f1e-8be2-f6047c5d47b2",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e33e6-5b30-484c-8160-701077aac12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems like our second best scores are with our ALS model, which will help address our issues of cold start. \n",
    "# Defining our ALS (Spark) model.\n",
    "als = ALS(\n",
    "    userCol='userId',\n",
    "    itemCol='movieId',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative= True\n",
    ")\n",
    "\n",
    "# Creating our parameter grid for our ALS spark model\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, [12, 13, 14])\\\n",
    "    .addGrid(als.maxIter, [18, 19, 20])\\\n",
    "    .addGrid(als.regParam, [.17, .18, .19])\\\n",
    "    .build()\n",
    "\n",
    "# Creating our Regression Evaluator using RMSE as our metric.\n",
    "\n",
    "reg_evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol ='rating',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "# Build cross validation using Train Validation Split\n",
    "\n",
    "train_val_split = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator= reg_evaluator)\n",
    "\n",
    "# Training our ALS model\n",
    "final_spark_model = train_val_split.fit(train)\n",
    "\n",
    "# Extracting the best model from the tuning process using ParamGridBuilder\n",
    "best_model = final_spark_model.bestModel\n",
    "\n",
    "# Generate predictions\n",
    "hold_out_predictions = final_spark_model.transform(hold_out)\n",
    "\n",
    "# Generate RMSE using our prediction and reg_evaluator\n",
    "hold_out_rmse = reg_evaluator.evaluate(hold_out_predictions)\n",
    "\n",
    "print(f\"\"\"  RSME = {hold_out_rmse} of current model.\n",
    "  **HOLD OUT MODEL**\n",
    "  Hyperparameters:\n",
    "  Rank: {best_model.rank}\n",
    "  Max Iterations: {best_model._java_obj.parent().getMaxIter()}\n",
    "  Regularization Parameter: {best_model._java_obj.parent().getRegParam()}\n",
    "  Nonnegative: {best_model._java_obj.parent().getNonnegative()}\n",
    "  Cold Start Strategy: {best_model._java_obj.parent().getColdStartStrategy()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6410e8d-71a6-4cd1-907a-88f8d1bb1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'ratings_df' is your pandas DataFrame\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Create the trainset\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Fit the final KNNBaseline model on the trainset\n",
    "final_knn_baseline = KNNBaseline(k=56, min_k=14, sim_options={'name': 'pearson', 'user_based': True})\n",
    "final_knn_baseline.fit(trainset)\n",
    "\n",
    "for i in cv_knn_baseline.items():\n",
    "    print(i)\n",
    "    \n",
    "print('-----------------------')\n",
    "# print validation results\n",
    "np.mean(final_cv_knn_baseline['test_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e86823-4a92-4278-944c-4ca900acf8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get movie recommendations so that we can evaluate our predictions as we develop our model.\n",
    "\n",
    "def get_movie_recommendations(userId=None, k=5):\n",
    "    if userId is None:\n",
    "        # Cold Start: Use ALS model\n",
    "        als_recommendations = best_model.recommendForAllUsers(k)\n",
    "\n",
    "        # Extract top k recommendations\n",
    "        top_recommendations = als_recommendations.select('userId', F.explode('recommendations').alias('recommendation')) \\\n",
    "                                                 .select('userId', 'recommendation.movieId', 'recommendation.rating')\n",
    "\n",
    "        # Convert to Pandas\n",
    "        top_recommendations = top_recommendations.toPandas()\n",
    "\n",
    "        # Join with movies_df for titles\n",
    "        top_recommendations = top_recommendations.merge(movies_df, on=\"movieId\", how=\"left\")\n",
    "\n",
    "        return top_recommendations[['movieId', 'title', 'rating']].sort_values(by='rating', ascending=False).head(k)\n",
    "\n",
    "    else:\n",
    "        # Known User: Use KNN model\n",
    "        neighbors = final_knn_baseline.get_neighbors(trainset.to_inner_uid(userId), k)\n",
    "        \n",
    "        # Map indices to movie IDs\n",
    "        movie_ids = [trainset.to_raw_iid(i) for i in neighbors]\n",
    "\n",
    "        # Predict ratings\n",
    "        predicted_ratings = [final_knn_baseline.predict(userId, movie_id).est for movie_id in movie_ids]\n",
    "\n",
    "        # Create DataFrame\n",
    "        recommendations_df = pd.DataFrame({'movieId': movie_ids, 'predicted_rating': predicted_ratings})\n",
    "\n",
    "        # Join with movies_df for titles\n",
    "        recommendations_df = recommendations_df.merge(movies_df, on=\"movieId\", how=\"left\")\n",
    "\n",
    "        # Get actual ratings (if available)\n",
    "        actual_ratings = ratings_df[ratings_df['userId'] == userId][['movieId', 'rating']].rename(columns={'rating': 'actual_rating'})\n",
    "        \n",
    "        # Merge actual ratings\n",
    "        recommendations_df = recommendations_df.merge(actual_ratings, on=\"movieId\", how=\"left\")\n",
    "\n",
    "        # Calculate prediction error\n",
    "        recommendations_df['error'] = abs(recommendations_df['predicted_rating'] - recommendations_df['actual_rating'])\n",
    "\n",
    "        # **Ensure cosine similarity column exists**\n",
    "        recommendations_df['cosine_similarity'] = None  # Default value\n",
    "\n",
    "        # Compute cosine similarity if no actual ratings exist\n",
    "        if recommendations_df['actual_rating'].isnull().all():\n",
    "            similarities = []\n",
    "            for movie_id in movie_ids:\n",
    "                inner_id = trainset.to_inner_iid(movie_id)\n",
    "                sim_scores = final_knn_baseline.sim[inner_id]\n",
    "                mean_similarity = sim_scores.mean()  # Get mean similarity\n",
    "                similarities.append(mean_similarity)\n",
    "\n",
    "            recommendations_df['cosine_similarity'] = similarities\n",
    "\n",
    "        # **Ensure column exists before selection**\n",
    "        columns_to_display = ['movieId', 'title', 'predicted_rating', 'actual_rating', 'error']\n",
    "        if 'cosine_similarity' in recommendations_df.columns:\n",
    "            columns_to_display.append('cosine_similarity')\n",
    "\n",
    "        # Display top k recommendations\n",
    "        return recommendations_df[columns_to_display].sort_values(by='predicted_rating', ascending=False).head(k)\n",
    "\n",
    "# Example usage:\n",
    "print(get_movie_recommendations(userId=40))  # For a known user\n",
    "print(get_movie_recommendations(userId=None))  # For cold start scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbe00d-0d29-4031-91fe-4a86cba2419b",
   "metadata": {},
   "source": [
    "### Our hybrid recommendation system works, with our KNN model providing an RMSE score of: 0.8649660909040028\n",
    "### while our ALS model has a score of: 0.8863465869481534"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801041dd-3e8f-4d8d-b9f3-e981c73e00f9",
   "metadata": {},
   "source": [
    "When a user has a sufficient user rating history, this model on average provides movies recommended to users within .88 of their predicted rating, however, in cases where the user has little to no user rating history or a movie no too few ratings, we cannot accurately predict what their rating will be. In such cases, our ALS model will be used to recommend movies based upon latent factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e67823-b831-4f1b-887a-d024468b0df5",
   "metadata": {},
   "source": [
    "During model development, we identified several data limitations that impact recommendation accuracy, particularly in addressing the cold-start problem. Additional user information—such as favorite genres, actors, or directors—could improve recommendations for new users. Tracking unrated movies, watch duration (e.g., whether a user finished a movie or stopped after a few minutes), and rewatch frequency would provide deeper insights into viewing habits. Incorporating these behavioral signals could enhance prediction accuracy and better capture user preferences beyond explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd15abb-ab8b-4cc2-8b69-7e80f4a821b6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9329df-8194-42a9-846d-335ed6992c43",
   "metadata": {},
   "source": [
    "Implementing a hybrid recommendation system, we combined the strengths of ALS and KNN Baseline to deliver personalized movie suggestions while addressing the cold-start problem. Our grid search optimization identified KNN as the best-performing model (RMSE = 0.886), outperforming the ALS baseline (RMSE = 0.888). While KNN excels for users with rich rating histories, ALS remains essential for recommending movies to new users or sparsely rated items.\n",
    "\n",
    "Further improvements could be made by incorporating additional user behavior data—such as watch duration, rewatch frequency, and unrated views—to refine predictions. With this hybrid approach, our client can maximize user engagement and deliver high-quality recommendations as their streaming platform grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b3421-4177-42c3-b6b2-c186a4f4b04b",
   "metadata": {},
   "source": [
    "For further details, please refer to the following linked project notebook and presentation:\n",
    "\n",
    "project notebook presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
